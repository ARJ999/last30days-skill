# Complete Revamp: Brave Search API + xAI Architecture

## Executive Summary

Replace the OpenAI Responses API dependency with Brave Search API (Pro Data AI plan) while retaining xAI for X/Twitter. This revamp transforms `/last30days` from a 3-source skill (Reddit via OpenAI, X via xAI, HN via Algolia) into a 7-source powerhouse (Reddit, X, HackerNews, News, Web, Videos, Discussions) with AI summaries, knowledge panels, FAQs, and schema-enriched structured data.

**What changes:**
- OpenAI Responses API (expensive, LLM-mediated, unreliable dates) is **removed entirely**
- Brave Search API becomes the primary web + Reddit + news + video + discussions engine
- xAI Responses API is **retained** as the sole X/Twitter source (Brave cannot search X)
- HackerNews Algolia is **retained** (free, precise, direct)
- Claude WebSearch fallback is **removed** (Brave replaces it with superior capabilities)

**Why this is strictly better:**

| Dimension | Current (OpenAI) | Revamp (Brave) |
|-----------|-------------------|----------------|
| Reddit discovery | LLM-mediated, hallucinated URLs/dates | Direct search engine, real URLs, `page_age` dates |
| Web search | Claude WebSearch fallback only | Native first-class source with `extra_snippets` |
| News | Not supported | Dedicated endpoint, up to 50 results |
| Videos | Not supported | Dedicated endpoint with metadata |
| Discussions | Not supported | Native forum/community thread discovery |
| Date filtering | LLM prompt engineering (40% accuracy) | `freshness` parameter + `page_age` field (deterministic) |
| Cost | Per-token LLM pricing ($$$) | $9/1K requests flat rate |
| Speed | LLM inference latency (3-10s) | Search API latency (<1s) |
| Reliability | LLM hallucinations, format parsing failures | Structured JSON, typed fields, deterministic |
| AI enrichment | None | Summarizer with citations, FAQs, infobox |
| Results per query | ~20-30 (LLM-limited) | Up to 200 (paginated) |
| Dependencies | OpenAI SDK/API key | Brave API key (one key for everything) |

---

## Current State Analysis

### Existing Sources and Their Problems

**1. Reddit (via OpenAI Responses API) â€” REMOVE**
- Module: `openai_reddit.py`
- Uses `web_search` tool with prompt engineering to find Reddit threads
- Problems:
  - LLM mediates all results â€” hallucinated URLs, fabricated dates, inconsistent formats
  - Only 40% of results verified within 30 days despite prompt instructions
  - Expensive: per-token billing on GPT-5+ models
  - Fragile: model access errors require fallback chain (`MODEL_FALLBACK_ORDER`)
  - Slow: 5-15s LLM inference time per query
  - Unreliable JSON parsing: must handle multiple response formats

**2. X/Twitter (via xAI Responses API) â€” KEEP**
- Module: `xai_x.py`
- Uses `x_search` tool â€” the only API that can search X/Twitter
- 100% date accuracy (xAI provides date in response)
- No alternative exists for X search

**3. HackerNews (via Algolia API) â€” KEEP**
- Module: `hn.py`
- Free, no API key needed
- Direct API with Unix timestamp filters â€” precise date range
- Always returns real data with points and discussion links

**4. WebSearch (via Claude built-in) â€” REMOVE**
- Module: `websearch.py`
- Fallback only, Claude-mediated, no engagement metrics
- Brave Web Search is strictly superior in every dimension

### Existing Bugs and Technical Debt

| Issue | Location | Status |
|-------|----------|--------|
| Report-level caching defined but never wired | `cache.py` / `last30days.py` | Unresolved |
| Duplicate `compute_hn_engagement_raw` | `hn.py:160` and `score.py:259` | Unresolved |
| `datetime.now()` vs UTC in websearch | `websearch.py:extract_date_from_snippet` | Unresolved |
| WebSearch items never pass through Python pipeline | `last30days.py` | By design but suboptimal |
| `--refresh` flag documented but not implemented | `SPEC.md` vs `last30days.py` | Unresolved |
| `best_practices` and `prompt_pack` fields always empty | `schema.py` | By design |
| No tests for `websearch.py`, `reddit_enrich.py`, `xai_x.py` parsing | `tests/` | Gap |
| O(n^2) dedup in deep mode | `dedupe.py` | Performance risk |

All of these are resolved by the revamp.

---

## Brave Search API Capability Map (Pro Data AI Plan)

### Endpoints Used in This Revamp

| Endpoint | URL | Purpose in Skill | Billing |
|----------|-----|------------------|---------|
| **Web Search** | `/res/v1/web/search` | General web + Reddit + discussions + FAQ + infobox + summarizer key + schema data | $9/1K |
| **News Search** | `/res/v1/news/search` | Dedicated news discovery, up to 50/page | $9/1K |
| **Video Search** | `/res/v1/videos/search` | Video content discovery | $9/1K |
| **Summarizer** | `/res/v1/summarizer/search` | AI summary with citations (two-step, uses key from web search) | Free (only web search billed) |

### Endpoints Not Used (and Why)

| Endpoint | Why Not Used |
|----------|-------------|
| Image Search | Not relevant to text-based research |
| Local POIs / Descriptions | Not relevant to topic research |
| Suggest / Spellcheck | Over-engineering; queries are user-provided |
| Rich Results | Weather/sports/stocks verticals not applicable |
| AI Grounding / Chat Completions | We use Claude for synthesis, not Brave's LLM |

### Key Parameters Leveraged

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `freshness` | `YYYY-MM-DDtoYYYY-MM-DD` | Deterministic 30-day window â€” eliminates the #1 bug |
| `extra_snippets` | `true` | Up to 5 additional contextual excerpts per result â€” richer data for scoring and synthesis |
| `summary` | `1` | Triggers summarizer key in response â€” enables AI summary with citations |
| `result_filter` | `discussions,faq,infobox` | Returns forum threads, FAQs, and knowledge panels in single call |
| `count` | `20` | Results per page (max for web search) |
| `offset` | `0-9` | Pagination for deeper research |
| `goggles` | Inline DSL | Custom re-ranking for quality filtering |
| `safesearch` | `off` | No content filtering (research tool) |

### Response Fields Leveraged

| Field | Source | Used For |
|-------|--------|----------|
| `web.results[].title` | Web Search | Item title |
| `web.results[].url` | Web Search | Item URL |
| `web.results[].description` | Web Search | Snippet / relevance context |
| `web.results[].page_age` | Web Search | **Deterministic date** â€” eliminates date guessing |
| `web.results[].age` | Web Search | Human-readable age string |
| `web.results[].extra_snippets[]` | Web Search | Up to 5 additional context excerpts |
| `web.results[].schemas` | Web Search | Schema.org structured data (articles, ratings, reviews) |
| `web.results[].meta_url` | Web Search | Domain, favicon, breadcrumb path |
| `web.results[].profile` | Web Search | Author/site profile |
| `web.results[].deep_results` | Web Search | Nested sub-results (social, news, videos) |
| `discussions.results[]` | Web Search | Forum/community discussion threads |
| `faq.results[]` | Web Search | Extracted FAQ question-answer pairs |
| `infobox` | Web Search | Knowledge panel entity data |
| `summarizer.key` | Web Search | Opaque key for summarizer endpoint |
| `news.results[].title/url/description/age` | News Search | News article data |
| `news.results[].extra_snippets[]` | News Search | Additional news excerpts |
| `videos.results[].title/url/description/age` | Video Search | Video content data |
| `videos.results[].thumbnail` | Video Search | Video thumbnail |

---

## Target Architecture

### High-Level Pipeline

```
User Query: /last30days [topic] for [tool]
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CONFIGURATION LAYER                              â”‚
â”‚  env.py â”€â”€â”€ Load BRAVE_API_KEY + XAI_API_KEY from ~/.config/last30days â”‚
â”‚  models.py â”€â”€â”€ Auto-select xAI model (Brave needs no model selection)  â”‚
â”‚  dates.py â”€â”€â”€ Compute from_date/to_date (30-day window, UTC)           â”‚
â”‚  cache.py â”€â”€â”€ Check report cache (24h TTL)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ (cache miss)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 1: PARALLEL SOURCE DISCOVERY (6 threads)             â”‚
â”‚                                                                         â”‚
â”‚  Thread 1: brave_web.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Brave Web Search (general)           â”‚
â”‚            q="{topic}", freshness=from..to, extra_snippets=true,        â”‚
â”‚            summary=1, result_filter=discussions,faq,infobox              â”‚
â”‚            Returns: web results + discussions + FAQ + infobox +          â”‚
â”‚                     summarizer_key + schema data                        â”‚
â”‚                                                                         â”‚
â”‚  Thread 2: brave_reddit.py â”€â”€â”€â”€â”€â”€â”€ Brave Web Search (Reddit-focused)    â”‚
â”‚            q="{topic} site:reddit.com", freshness=from..to,             â”‚
â”‚            extra_snippets=true, count=20                                â”‚
â”‚            Returns: Reddit thread URLs with page_age dates              â”‚
â”‚                                                                         â”‚
â”‚  Thread 3: brave_news.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Brave News Search                   â”‚
â”‚            q="{topic}", freshness=from..to, extra_snippets=true,        â”‚
â”‚            count=50                                                     â”‚
â”‚            Returns: up to 50 news articles with dates                   â”‚
â”‚                                                                         â”‚
â”‚  Thread 4: brave_video.py â”€â”€â”€â”€â”€â”€â”€â”€ Brave Video Search                   â”‚
â”‚            q="{topic}", freshness=from..to, count=20                    â”‚
â”‚            Returns: video results with thumbnails/metadata              â”‚
â”‚                                                                         â”‚
â”‚  Thread 5: xai_x.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ xAI X Search (RETAINED)               â”‚
â”‚            Uses x_search tool with date range                           â”‚
â”‚            Returns: X posts with engagement metrics                     â”‚
â”‚                                                                         â”‚
â”‚  Thread 6: hn.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HN Algolia (RETAINED)                  â”‚
â”‚            numericFilters for date range                                â”‚
â”‚            Returns: HN stories with points                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 2: ENRICHMENT (parallel, 5 threads)                  â”‚
â”‚                                                                         â”‚
â”‚  reddit_enrich.py â”€â”€â”€â”€ Fetch Reddit thread JSON for URLs found in       â”‚
â”‚                        Phase 1 Thread 2 (verified engagement metrics,   â”‚
â”‚                        real upvotes, comments, top comment excerpts)     â”‚
â”‚                                                                         â”‚
â”‚  brave_summarizer.py â”€â”€ Use summarizer_key from Phase 1 Thread 1        â”‚
â”‚                         to fetch AI summary with citations               â”‚
â”‚                         + follow-up questions                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 3: PROCESSING PIPELINE                               â”‚
â”‚                                                                         â”‚
â”‚  normalize.py â”€â”€â”€â”€ Raw API responses â†’ typed dataclasses                â”‚
â”‚       â”‚            (RedditItem, XItem, HNItem, NewsItem, WebItem,       â”‚
â”‚       â”‚             VideoItem, DiscussionItem)                          â”‚
â”‚       â–¼                                                                 â”‚
â”‚  normalize.filter_by_date_range() â”€â”€â”€â”€ Hard filter (defense-in-depth)   â”‚
â”‚       â”‚            Brave's freshness param is primary filter;            â”‚
â”‚       â”‚            this is a safety net for edge cases                   â”‚
â”‚       â–¼                                                                 â”‚
â”‚  score.py â”€â”€â”€â”€ Source-specific weighted scoring                         â”‚
â”‚       â”‚        Reddit/X/HN: 40% relevance + 25% recency + 35% engage  â”‚
â”‚       â”‚        News: 45% relevance + 55% recency                       â”‚
â”‚       â”‚        Web: 55% relevance + 45% recency - 10pt penalty         â”‚
â”‚       â”‚        Videos: 50% relevance + 50% recency                     â”‚
â”‚       â”‚        Discussions: 45% relevance + 25% recency + 30% engage   â”‚
â”‚       â–¼                                                                 â”‚
â”‚  dedupe.py â”€â”€â”€â”€ Within-source + cross-source deduplication              â”‚
â”‚       â”‚         URL-based (exact match) then content-based (Jaccard)    â”‚
â”‚       â–¼                                                                 â”‚
â”‚  schema.py â”€â”€â”€â”€ Assemble Report with all sources + enrichment data      â”‚
â”‚                 Compute DataQuality metrics                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 4: OUTPUT                                            â”‚
â”‚                                                                         â”‚
â”‚  cache.py â”€â”€â”€â”€ Save report to 24h cache                                 â”‚
â”‚  render.py â”€â”€â”€â”€ Format output (compact/json/md/context/path)            â”‚
â”‚  ui.py â”€â”€â”€â”€ Terminal progress display                                   â”‚
â”‚  stdout â”€â”€â”€â”€ Claude consumes compact output for synthesis               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Source Strategy Matrix

| Source | API | Query Strategy | Date Filtering | Engagement Source | Items (quick/default/deep) |
|--------|-----|----------------|----------------|-------------------|---------------------------|
| **Reddit** | Brave Web Search | `"{topic} site:reddit.com"` | `freshness=YYYY-MM-DDtoYYYY-MM-DD` | Reddit JSON API (enrichment) | 10 / 20-40 / 60 |
| **X/Twitter** | xAI Responses API | `x_search` tool | Date range in prompt | xAI response (native) | 8-12 / 20-30 / 40-60 |
| **HackerNews** | Algolia API | REST query | `numericFilters` (Unix timestamps) | Algolia response (native) | 10-15 / 20-30 / 40-60 |
| **News** | Brave News Search | `"{topic}"` | `freshness=YYYY-MM-DDtoYYYY-MM-DD` | None (recency-weighted) | 10-15 / 25-50 / 50 |
| **Web** | Brave Web Search | `"{topic}"` | `freshness=YYYY-MM-DDtoYYYY-MM-DD` | Schema data (if available) | 10-15 / 20-40 / 60 |
| **Videos** | Brave Video Search | `"{topic}"` | `freshness=YYYY-MM-DDtoYYYY-MM-DD` | None (recency-weighted) | 5-10 / 10-20 / 20-40 |
| **Discussions** | Brave Web Search | `result_filter=discussions` | `freshness=YYYY-MM-DDtoYYYY-MM-DD` | Brave response (limited) | Included in web search |

### Pagination Strategy

Brave Web Search returns `count=20` per page with `offset=0-9` (max 200 results).

| Depth | Pages per Endpoint | Total Results Fetched | Used After Scoring |
|-------|--------------------|-----------------------|--------------------|
| `--quick` | 1 | 20 per source | Top 10-15 |
| default | 2 | 40 per source | Top 20-30 |
| `--deep` | 3 | 60 per source | Top 40-60 |

Pagination is applied to: Brave Web (general), Brave Web (Reddit), Brave News. Video search and HN/xAI are single-call.

For paginated sources, requests are sequential per source (page N+1 depends on confirming `query.more_results_available=true` from page N) but all sources run in parallel.

### Goggles Strategy

Goggles provide custom re-ranking without additional API calls.

**Default Goggles for general web search:**
```
$discard,site=pinterest.com
$discard,site=quora.com
$boost=2,site=github.com
$boost=2,site=dev.to
$boost=2,site=stackoverflow.com
```

Rationale: Pinterest and Quora are low-signal for research. GitHub, dev.to, and StackOverflow are high-signal for technical topics.

**No Goggles for Reddit-focused search** (already filtered by `site:reddit.com`).

**No Goggles for News/Video** (dedicated endpoints handle ranking).

Goggles are passed as inline DSL via the `goggles` parameter. No hosted file needed.

### Search Operator Strategy

Brave supports search operators within the `q` parameter:

| Search Type | Query Construction | Operators Used |
|-------------|-------------------|----------------|
| Reddit | `"{topic} site:reddit.com"` | `site:` restricts to reddit.com |
| General Web | `"{topic}"` | None (let Brave rank freely) |
| News | `"{topic}"` | None |
| Videos | `"{topic}"` | None |

For retry with insufficient results:
- Simplify topic to core 2-3 terms
- Remove quotes for broader matching
- Add `-site:reddit.com -site:twitter.com -site:x.com` to general web (avoid source overlap)

---

## Module Design

### New Modules

#### `scripts/lib/brave_client.py` â€” Shared Brave API Client

Single HTTP client for all Brave endpoints. Handles authentication, rate limiting, error handling, and response parsing.

```
Dependencies: http.py (stdlib HTTP client)
Exports:
  - BraveClient class
    - __init__(api_key: str)
    - web_search(q, freshness, count, offset, extra_snippets, summary, result_filter, goggles) -> Dict
    - news_search(q, freshness, count, offset, extra_snippets) -> Dict
    - video_search(q, freshness, count, offset) -> Dict
    - summarizer_search(key, inline_references) -> Dict
  - BraveError(Exception)
    - status_code: int
    - error_code: str (e.g., "SUBSCRIPTION_TOKEN_INVALID")
```

**Design decisions:**
- Reuses `http.py` for stdlib-only HTTP (no external dependencies)
- Adds `X-Subscription-Token` header to all requests
- Parses rate limit headers (`X-RateLimit-Remaining`, `X-RateLimit-Reset`)
- Implements 429 backoff: respects `Retry-After` header, exponential 1s/2s/4s/8s
- Does NOT retry 4xx (except 429) â€” fail fast on auth/param errors
- Retries 5xx and connection errors (same as existing `http.py` logic)

#### `scripts/lib/brave_web.py` â€” General Web Search + Discussions + FAQ + Infobox + Summarizer Key

Handles the primary Brave Web Search call that returns multiple result types in a single response.

```
Dependencies: brave_client.py, dates.py
Exports:
  - search_web(client, topic, from_date, to_date, depth, goggles) -> Dict
      Returns raw Brave response with web + discussions + FAQ + infobox + summarizer key
  - parse_web_results(response) -> List[Dict]
      Extracts web results, assigns IDs (W1, W2...), maps page_age to date
  - parse_discussions(response) -> List[Dict]
      Extracts discussion results, assigns IDs (D1, D2...), identifies forum source
  - parse_faq(response) -> List[Dict]
      Extracts FAQ question-answer pairs
  - parse_infobox(response) -> Optional[Dict]
      Extracts knowledge panel data
  - get_summarizer_key(response) -> Optional[str]
      Extracts opaque summarizer key
```

**Pagination logic:**
```
page 1: offset=0, count=20
  check response.query.more_results_available
page 2: offset=1, count=20
  check response.query.more_results_available
page 3: offset=2, count=20 (deep only)
```

Results from all pages are merged before parsing.

#### `scripts/lib/brave_reddit.py` â€” Reddit Discovery via Brave

Handles Reddit-specific search using Brave Web Search with `site:reddit.com`.

```
Dependencies: brave_client.py, dates.py
Exports:
  - search_reddit(client, topic, from_date, to_date, depth) -> Dict
      Returns raw Brave response filtered to reddit.com
  - parse_reddit_results(response) -> List[Dict]
      Extracts Reddit thread URLs, assigns IDs (R1, R2...),
      maps page_age to date, extracts subreddit from URL
```

**Query construction:**
```
Primary:   "{topic} site:reddit.com"
Retry:     "{core_terms} site:reddit.com" (simplified 2-3 word topic)
```

**Subreddit extraction:**
```python
# From URL: https://www.reddit.com/r/ClaudeCode/comments/abc123/title/
# Extract: "ClaudeCode"
match = re.search(r'reddit\.com/r/([^/]+)', url)
subreddit = match.group(1) if match else "unknown"
```

#### `scripts/lib/brave_news.py` â€” News Discovery

Handles dedicated Brave News Search endpoint.

```
Dependencies: brave_client.py, dates.py
Exports:
  - search_news(client, topic, from_date, to_date, depth) -> Dict
      Returns raw Brave News response (up to 50 per page)
  - parse_news_results(response) -> List[Dict]
      Extracts news articles, assigns IDs (N1, N2...),
      maps age/page_age to date, extracts source name/domain
```

**Key difference from web search:** News endpoint supports `count=50` (vs 20 for web), providing denser news coverage in fewer API calls.

#### `scripts/lib/brave_video.py` â€” Video Discovery

Handles dedicated Brave Video Search endpoint.

```
Dependencies: brave_client.py, dates.py
Exports:
  - search_videos(client, topic, from_date, to_date, depth) -> Dict
      Returns raw Brave Video response
  - parse_video_results(response) -> List[Dict]
      Extracts videos, assigns IDs (V1, V2...),
      maps age to date, extracts creator/thumbnail/duration
```

#### `scripts/lib/brave_summarizer.py` â€” AI Summary with Citations

Handles the two-step Brave Summarizer flow.

```
Dependencies: brave_client.py
Exports:
  - fetch_summary(client, summarizer_key) -> Optional[Dict]
      Step 2: Uses key from web search to get AI summary
      Returns: {"summary": str, "citations": List[Dict], "followups": List[str]}
  - parse_summary_response(response) -> Dict
      Extracts summary text, inline citation references, follow-up questions
```

**Two-step flow:**
1. Phase 1 web search includes `summary=1` â†’ response includes `summarizer.key`
2. Phase 2 calls `/res/v1/summarizer/search?key={key}&inline_references=true`
3. Summarizer request is **not billed** â€” only the web search counts

**Failure handling:** If summarizer key is absent (topic too niche) or summarizer call fails, skip gracefully. Summary is enrichment, not critical path.

### Modified Modules

#### `scripts/lib/env.py` â€” Configuration Updates

**Changes:**
- Replace `OPENAI_API_KEY` with `BRAVE_API_KEY`
- Remove `OPENAI_MODEL_POLICY` and `OPENAI_MODEL_PIN`
- Update `get_available_sources()` logic
- Update `validate_sources()` for new source combinations

**New configuration keys:**
```bash
# ~/.config/last30days/.env
BRAVE_API_KEY=BSA...           # Required for web/Reddit/news/video/discussions
XAI_API_KEY=xai-...            # Required for X/Twitter
XAI_MODEL_POLICY=latest        # 'latest' or 'stable'
XAI_MODEL_PIN=grok-4-1         # Optional specific model
```

**New source availability matrix:**

| Keys Present | `get_available_sources()` | Available Sources |
|---|---|---|
| BRAVE + XAI | `'full'` | Reddit + X + HN + News + Web + Videos + Discussions + Summarizer |
| BRAVE only | `'brave'` | Reddit + HN + News + Web + Videos + Discussions + Summarizer |
| XAI only | `'x'` | X + HN |
| None | `'hn'` | HN only (free, always available) |

**Key improvement:** With just a single Brave API key, users get 6+ source types vs the old model needing two separate API keys (OpenAI + xAI) for 2 sources.

#### `scripts/lib/models.py` â€” Simplification

**Changes:**
- Remove all OpenAI model selection logic (`select_openai_model`, `is_mainline_openai_model`, `parse_version`, `OPENAI_MODELS_URL`, `OPENAI_FALLBACK_MODELS`)
- Keep xAI model selection unchanged (`select_xai_model`, `is_grok_search_capable`, `XAI_ALIASES`)
- Rename `get_models()` to reflect single-provider model selection
- Remove OpenAI model cache entries from `cache.py`

**Resulting module is ~60% smaller.** Brave Search requires no model selection â€” it's a search API, not an LLM.

#### `scripts/lib/schema.py` â€” New Item Types + Updated Report

**New dataclasses:**

```python
@dataclass
class NewsItem:
    id: str                          # N1, N2, etc.
    title: str
    url: str
    source_name: str                 # e.g., "TechCrunch"
    source_domain: str               # e.g., "techcrunch.com"
    date: Optional[str]              # YYYY-MM-DD from page_age
    date_confidence: str             # 'high' (Brave provides page_age)
    snippet: str
    extra_snippets: List[str]        # Up to 5 from Brave
    relevance: float
    why_relevant: str                # Auto-generated from snippet
    subs: SubScores
    score: int

@dataclass
class VideoItem:
    id: str                          # V1, V2, etc.
    title: str
    url: str
    source_domain: str               # e.g., "youtube.com"
    creator: Optional[str]           # Video creator/channel
    date: Optional[str]
    date_confidence: str
    thumbnail_url: Optional[str]
    duration: Optional[str]          # If available from metadata
    snippet: str
    relevance: float
    why_relevant: str
    subs: SubScores
    score: int

@dataclass
class DiscussionItem:
    id: str                          # D1, D2, etc.
    title: str
    url: str
    forum_name: str                  # e.g., "Stack Overflow", "Hacker News"
    date: Optional[str]
    date_confidence: str
    snippet: str
    extra_snippets: List[str]
    relevance: float
    why_relevant: str
    subs: SubScores
    score: int
```

**Updated `Report` dataclass:**

```python
@dataclass
class Report:
    topic: str
    range_from: str
    range_to: str
    generated_at: str
    mode: str

    # Models (only xAI now)
    xai_model: Optional[str] = None

    # Primary social sources (engagement-verified)
    reddit: List[RedditItem] = field(default_factory=list)
    x: List[XItem] = field(default_factory=list)
    hn: List[HNItem] = field(default_factory=list)

    # Brave-powered sources
    news: List[NewsItem] = field(default_factory=list)
    web: List[WebItem] = field(default_factory=list)
    videos: List[VideoItem] = field(default_factory=list)
    discussions: List[DiscussionItem] = field(default_factory=list)

    # Brave enrichment data
    summary: Optional[str] = None
    summary_citations: List[Dict] = field(default_factory=list)
    summary_followups: List[str] = field(default_factory=list)
    infobox: Optional[Dict] = None
    faqs: List[Dict] = field(default_factory=list)

    # Errors per source
    reddit_error: Optional[str] = None
    x_error: Optional[str] = None
    hn_error: Optional[str] = None
    news_error: Optional[str] = None
    web_error: Optional[str] = None
    video_error: Optional[str] = None

    # Cache info
    from_cache: bool = False
    cache_age_hours: Optional[float] = None

    # Quality metrics
    data_quality: Optional[DataQuality] = None
```

**Updated `DataQuality`:**

```python
@dataclass
class DataQuality:
    total_items: int
    verified_dates_count: int        # Items with page_age from Brave (should be ~100%)
    verified_engagement_count: int   # Reddit (enriched) + X + HN
    avg_recency_days: Optional[float]
    sources_available: List[str]     # e.g., ["reddit", "x", "hn", "news", "web", "videos"]
    sources_failed: List[str]
    has_summary: bool                # Whether Brave Summarizer produced output
    has_infobox: bool                # Whether knowledge panel was returned
    faq_count: int                   # Number of FAQ pairs extracted
```

**Removed from `Report`:** `openai_model` field (no longer applicable).

**Renamed:** `WebSearchItem` â†’ `WebItem` (no longer "search" â€” it's a direct Brave result).

#### `scripts/lib/normalize.py` â€” New Normalizers

**New functions:**
- `normalize_news_items(items, from_date, to_date) -> List[NewsItem]`
- `normalize_web_items(items, from_date, to_date) -> List[WebItem]`
- `normalize_video_items(items, from_date, to_date) -> List[VideoItem]`
- `normalize_discussion_items(items, from_date, to_date) -> List[DiscussionItem]`

**Updated functions:**
- `normalize_reddit_items` â€” adapted for Brave response format (URLs from Brave, enrichment from Reddit JSON API)
- `filter_by_date_range` â€” extended to handle all new item types

**Key improvement:** Brave provides `page_age` for virtually every result, so `date_confidence='high'` will be the default instead of `'low'`. This eliminates the major date reliability problem.

#### `scripts/lib/score.py` â€” New Scoring Formulas

**Relevance scoring for Brave results (position-based):**

Brave returns results ordered by its search algorithm. Position IS relevance.

```python
def compute_position_relevance(position: int, total: int) -> float:
    """Compute relevance from search result position.

    Position 0 = 1.0, last position = 0.2, linear decay.
    """
    if total <= 1:
        return 1.0
    return max(0.2, 1.0 - (position / total) * 0.8)
```

**Source-specific scoring formulas:**

| Source | Relevance | Recency | Engagement | Penalties/Bonuses |
|--------|-----------|---------|------------|-------------------|
| Reddit | 40% | 25% | 35% (enriched) | +8 verified engagement, -15 unverified |
| X | 40% | 25% | 35% (native) | +8 verified, -15 unverified |
| HN | 40% | 25% | 35% (native) | Always verified |
| News | 45% | 55% | 0% | +5 extra_snippets present |
| Web | 55% | 45% | 0% | -10 source penalty, +5 schema data, +3 extra_snippets |
| Videos | 50% | 50% | 0% | No penalties |
| Discussions | 45% | 25% | 30% (if available) | -5 if no engagement data |

**Rationale for news scoring:** News articles are valued primarily for freshness (55% recency) â€” a 2-day-old news article is much more valuable than a 25-day-old one. Relevance (45%) ensures topicality.

**Rationale for web scoring:** General web results lack engagement signals, so relevance dominates (55%). The -10 source penalty ensures web results rank below equivalent Reddit/X/HN items that have verified community engagement. Schema data bonus (+5) rewards structured content. Extra snippets bonus (+3) rewards content-rich results.

**New functions:**
- `score_news_items(items) -> List[NewsItem]`
- `score_web_items(items) -> List[WebItem]` (replaces `score_websearch_items`)
- `score_video_items(items) -> List[VideoItem]`
- `score_discussion_items(items) -> List[DiscussionItem]`

**Removed functions:**
- `score_websearch_items` â€” replaced by `score_web_items`

#### `scripts/lib/dedupe.py` â€” Cross-Source Deduplication

**New capability: URL-based cross-source dedup before content-based dedup.**

```
Step 1: URL dedup (exact match after normalization)
  - Normalize: lowercase, strip trailing /, strip query params, strip www.
  - If same URL in Reddit + Discussions: keep Reddit (has enrichment)
  - If same URL in Web + News: keep News (more specific)
  - If same URL in any other combination: keep highest-scored

Step 2: Content dedup (Jaccard, within-source, existing logic)
  - Applied per source type after URL dedup
  - Threshold: 0.7 (unchanged)
```

**New functions:**
- `dedupe_cross_source(all_items: Dict[str, List]) -> Dict[str, List]`
  - Takes `{"reddit": [...], "x": [...], "news": [...], "web": [...], ...}`
  - Returns same structure with URL duplicates removed
  - Priority order for keep: Reddit > X > HN > News > Discussions > Web > Videos
- `dedupe_news(items, threshold=0.7) -> List[NewsItem]`
- `dedupe_web(items, threshold=0.7) -> List[WebItem]`
- `dedupe_videos(items, threshold=0.7) -> List[VideoItem]`
- `dedupe_discussions(items, threshold=0.7) -> List[DiscussionItem]`

**Performance optimization for deep mode:**
- Existing O(n^2) Jaccard is acceptable for n<=60 per source (~1800 comparisons max)
- Cross-source URL dedup is O(n) using a URLâ†’source_priority dict
- No performance concern at planned volumes

#### `scripts/lib/cache.py` â€” Wire Up Report Caching

**Changes:**
- Wire `load_cache`/`save_cache` into `last30days.py` main flow (currently defined but never called)
- Update cache key to include all source parameters: `SHA256(topic + from_date + to_date + sources + depth)`
- Add `--refresh` CLI flag to bypass cache
- Remove OpenAI model cache entries
- Keep xAI model cache

**Cache flow:**
```
1. Compute cache_key from (topic, from_date, to_date, sources, depth)
2. Check load_cache(cache_key, ttl_hours=24)
3. If hit: deserialize Report from JSON, set from_cache=True, skip phases 1-3
4. If miss: run full pipeline, save_cache(cache_key, report.to_dict())
5. --refresh flag: skip step 2-3, always run full pipeline
```

#### `scripts/lib/render.py` â€” New Output Sections

**Updated `render_compact`:**
```
## Research Report: {topic}
Date range: {from_date} to {to_date}

### AI Summary (Brave Summarizer)
{summary_text_with_citations}

### Knowledge Panel
{infobox_data_if_present}

### Reddit ({n} threads, {sum_upvotes} upvotes)
R1. [score:{s}] {title} â€” r/{subreddit} â€” {date} â€” {upvotes}â†‘ {comments}ðŸ’¬
    Top insight: {comment_excerpt}

### X ({n} posts, {sum_likes} likes)
X1. [score:{s}] {text_truncated} â€” @{handle} â€” {date} â€” {likes}â™¥ {reposts}ðŸ”„

### HackerNews ({n} stories, {sum_points} points)
HN1. [score:{s}] {title} â€” {date} â€” {points}pts

### News ({n} articles)
N1. [score:{s}] {title} â€” {source_name} â€” {date}
    {snippet}

### Web ({n} pages)
W1. [score:{s}] {title} â€” {source_domain} â€” {date}
    {snippet}

### Videos ({n} videos)
V1. [score:{s}] {title} â€” {source_domain} â€” {date}
    {creator}

### Discussions ({n} threads)
D1. [score:{s}] {title} â€” {forum_name} â€” {date}
    {snippet}

### FAQs
Q: {question}
A: {answer}

### Data Quality
Total: {n} items | Verified dates: {n}% | Verified engagement: {n}%
Sources: {list} | Failed: {list}
```

**Updated `render_context_snippet`:** Top 10 items across all 7 sources + summary excerpt.

**Updated `write_outputs`:**
- `report.json` â€” full report with all sources
- `report.md` â€” human-readable with all sections
- `last30days.context.md` â€” compact context snippet
- `raw_brave_web.json` â€” raw Brave Web Search response
- `raw_brave_reddit.json` â€” raw Brave Reddit-focused response
- `raw_brave_news.json` â€” raw Brave News response
- `raw_brave_video.json` â€” raw Brave Video response
- `raw_brave_summary.json` â€” raw Brave Summarizer response
- `raw_xai.json` â€” raw xAI response (unchanged)
- `raw_hn.json` â€” raw HN Algolia response
- `raw_reddit_threads_enriched.json` â€” enriched Reddit thread data (unchanged)

#### `scripts/lib/ui.py` â€” New Source Progress Displays

**New methods on `ProgressDisplay`:**
- `start_news()` / `end_news(count)`
- `start_web()` / `end_web(count)`
- `start_videos()` / `end_videos(count)`
- `start_summarizer()` / `end_summarizer()`

**New message arrays:**
- `NEWS_MESSAGES` â€” fun messages for news search phase
- `WEB_MESSAGES` â€” fun messages for web search phase (replace `WEB_ONLY_MESSAGES`)
- `VIDEO_MESSAGES` â€” fun messages for video search phase
- `SUMMARIZER_MESSAGES` â€” fun messages for summarizer phase

**Updated `show_complete`:** Shows all 7 source counts.

**Updated promo messages:** Reflect new API key structure (Brave + xAI).

#### `scripts/lib/http.py` â€” No Changes

The existing stdlib HTTP client is reused by `brave_client.py`. No modifications needed.

#### `scripts/lib/dates.py` â€” Minor Fix

**Fix:** Ensure all date functions use `datetime.now(timezone.utc)` consistently. The existing `recency_score` already uses UTC. No other changes needed since Brave provides `page_age` directly.

#### `scripts/lib/reddit_enrich.py` â€” No Changes

Reddit enrichment logic is unchanged. It fetches the actual Reddit JSON API for thread engagement metrics. The only difference is the URLs now come from Brave (reliable, real URLs) instead of OpenAI (potentially hallucinated URLs).

**Expected improvement:** Enrichment success rate increases because Brave returns real, verified Reddit URLs while OpenAI sometimes returned hallucinated or malformed URLs.

### Removed Modules

| Module | Reason |
|--------|--------|
| `scripts/lib/openai_reddit.py` | Replaced by `brave_reddit.py` â€” Brave provides superior Reddit discovery |
| `scripts/lib/websearch.py` | Replaced by `brave_web.py` â€” Brave provides superior web search |

**Important:** The date extraction functions in `websearch.py` (`extract_date_from_url`, `extract_date_from_snippet`) are **no longer needed** because Brave provides `page_age` natively for all results. This eliminates an entire class of date-parsing complexity and bugs.

### Orchestrator Rewrite: `scripts/last30days.py`

**Major changes:**

1. **CLI arguments:**
   - Remove `--include-web` (web is always included when Brave key present)
   - Add `--refresh` (bypass cache)
   - Update `--sources` choices: `auto|reddit|x|news|web|all` (default: auto)
   - Keep `--mock`, `--emit`, `--quick`, `--deep`, `--debug`

2. **Initialization:**
   - Load `BRAVE_API_KEY` and `XAI_API_KEY` from env
   - Create `BraveClient` instance if Brave key present
   - Auto-select xAI model if xAI key present (unchanged)

3. **`run_research()` rewrite:**
   - `ThreadPoolExecutor(max_workers=6)` for Phase 1 (was 3)
   - Submit all 6 source searches in parallel
   - Collect results with timeouts
   - `ThreadPoolExecutor(max_workers=5)` for Phase 2 enrichment (unchanged)
   - Reddit enrichment + summarizer fetch in parallel

4. **Processing pipeline:**
   - Normalize all 7 source types
   - Hard date filter all sources (defense-in-depth)
   - Score all sources with source-specific formulas
   - Cross-source URL dedup, then within-source content dedup
   - Sort all items by score descending
   - Assemble Report

5. **WebSearch marker removed:**
   - No more `### WEBSEARCH REQUIRED ###` stdout marker
   - Claude no longer needs to do WebSearch â€” Brave handles everything

---

## SKILL.md Interface Changes

### Updated Frontmatter

```yaml
---
name: last30days
description: Research a topic from the last 30 days across Reddit + X + News + Web + Videos, become an expert, and write copy-paste-ready prompts.
argument-hint: "[topic] for [tool]" or "[topic]"
context: fork
agent: Explore
disable-model-invocation: true
allowed-tools: Bash, Read, Write, AskUserQuestion
---
```

**Changes:**
- `WebSearch` removed from `allowed-tools` (Brave replaces it)
- Description updated to reflect all sources

### Updated Research Execution

**Simplified to single step:**

```bash
python3 ~/.claude/skills/last30days/scripts/last30days.py "$ARGUMENTS" --emit=compact 2>&1
```

**No more Step 2 (WebSearch) or Step 3 (wait for background).** The Python script handles everything via Brave API. Claude only needs to:
1. Run the script
2. Read the output
3. Synthesize and present to user

### Updated Stats Display

```
---
All agents reported back!
â”œâ”€ Reddit: {n} threads | {sum} upvotes | {sum} comments
â”œâ”€ X: {n} posts | {sum} likes | {sum} reposts
â”œâ”€ HN: {n} stories | {sum} points
â”œâ”€ News: {n} articles | {sources}
â”œâ”€ Web: {n} pages | {domains}
â”œâ”€ Videos: {n} videos
â”œâ”€ Discussions: {n} forum threads
â”œâ”€ AI Summary: {present/absent} | FAQ: {n} pairs
â””â”€ Top voices: r/{sub1}, r/{sub2} | @{handle1} | {news_source}
```

### Removed Sections

- "Step 3: Do WebSearch" â€” no longer needed
- Web-only mode messaging â€” minimum mode is now "HN only" (always free)
- Promo banners for missing OpenAI key â€” replaced with Brave key promo

---

## Configuration Changes

### New Configuration File

```bash
# ~/.config/last30days/.env

# Brave Search API (Pro Data AI plan)
# Get your key at: https://api-dashboard.search.brave.com
BRAVE_API_KEY=BSA...

# xAI API (for X/Twitter research)
# Get your key at: https://console.x.ai
XAI_API_KEY=xai-...

# xAI Model Selection (optional)
XAI_MODEL_POLICY=latest          # 'latest' or 'stable'
# XAI_MODEL_PIN=grok-4-1         # Pin to specific model
```

### Migration from Old Configuration

Users with existing config need to:
1. Remove `OPENAI_API_KEY`
2. Remove `OPENAI_MODEL_POLICY` and `OPENAI_MODEL_PIN`
3. Add `BRAVE_API_KEY`
4. Keep `XAI_API_KEY` unchanged

**The skill will detect old config and print a migration message:**
```
[MIGRATION] Your config uses OPENAI_API_KEY which is no longer needed.
Please replace it with BRAVE_API_KEY for superior research capabilities.
Run: ~/.claude/skills/last30days/scripts/migrate_config.sh
```

---

## Error Handling and Graceful Degradation

### Source Failure Matrix

| Failure | Degradation | User-Visible Impact |
|---------|-------------|---------------------|
| Brave Web Search fails | Skip web/discussions/FAQ/infobox/summarizer | No general web results |
| Brave Reddit search fails | Reddit items empty, no enrichment | No Reddit data |
| Brave News fails | Skip news section | No news articles |
| Brave Videos fails | Skip videos section | No video results |
| Brave Summarizer fails | Skip summary section | No AI summary (minor) |
| xAI fails | Skip X section | No X/Twitter data |
| HN fails | Skip HN section | No HN data |
| Reddit enrichment fails (per-thread) | Keep Brave discovery data, `engagement_verified=false` | Reduced engagement accuracy |
| All Brave fails | HN (free) + X (if key) only | Severely reduced coverage |
| All sources fail | Error message, no report | Complete failure |

### Error Handling Pattern

```python
# Every source follows this pattern:
try:
    raw_response = source_module.search(...)
    items = source_module.parse(raw_response)
except BraveError as e:
    if e.status_code == 401:
        error = "Invalid BRAVE_API_KEY. Check your key at api-dashboard.search.brave.com"
    elif e.status_code == 403:
        error = "Brave plan does not include this feature. Pro Data AI plan required."
    elif e.status_code == 429:
        error = "Rate limit exceeded. Will retry with backoff."
        # Retry logic in brave_client.py
    else:
        error = f"Brave API error {e.status_code}: {e.error_code}"
    items = []
    report.{source}_error = error
except Exception as e:
    error = f"{type(e).__name__}: {e}"
    items = []
    report.{source}_error = error
```

### Rate Limit Handling

Brave Pro Data AI plan: 50 req/s, unlimited monthly.

With 6 parallel Phase 1 searches + potential pagination (up to 3 pages each = 18 requests) + Phase 2 enrichment, maximum burst is ~25 requests in ~2 seconds. Well within rate limits.

If 429 occurs (shared rate limit with other consumers of the same API key):
1. Read `X-RateLimit-Reset` header
2. Wait specified seconds
3. Retry with exponential backoff: 1s, 2s, 4s, 8s
4. Max 4 retries per request

---

## Testing Strategy

### New Fixtures

| Fixture File | Source | Content |
|---|---|---|
| `brave_web_sample.json` | Brave Web Search | Web results + discussions + FAQ + infobox + summarizer key |
| `brave_reddit_sample.json` | Brave Web Search (Reddit) | Reddit-specific results with page_age |
| `brave_news_sample.json` | Brave News Search | 50 news articles |
| `brave_video_sample.json` | Brave Video Search | 20 video results |
| `brave_summarizer_sample.json` | Brave Summarizer | AI summary with citations + follow-ups |

**Kept fixtures:** `xai_sample.json`, `hn_sample.json`, `reddit_thread_sample.json`, `models_xai_sample.json`

**Removed fixtures:** `openai_sample.json`, `models_openai_sample.json` (no longer needed)

### New Test Files

| Test File | Coverage |
|---|---|
| `test_brave_client.py` | BraveClient construction, auth header, error mapping, rate limit parsing |
| `test_brave_web.py` | Web result parsing, discussion parsing, FAQ parsing, infobox parsing, summarizer key extraction, pagination logic |
| `test_brave_reddit.py` | Reddit URL extraction, subreddit parsing, page_age to date mapping |
| `test_brave_news.py` | News result parsing, source name extraction, extra_snippets handling |
| `test_brave_video.py` | Video result parsing, creator/thumbnail extraction |
| `test_brave_summarizer.py` | Summary parsing, citation extraction, follow-up parsing, graceful failure |
| `test_score_new.py` | `score_news_items`, `score_web_items`, `score_video_items`, `score_discussion_items`, `compute_position_relevance` |
| `test_dedupe_cross.py` | Cross-source URL dedup, priority ordering, merge behavior |
| `test_env_new.py` | `get_available_sources` with Brave/xAI combinations, `validate_sources` |
| `test_integration.py` | End-to-end mock mode: full pipeline from CLI args to rendered output |

### Updated Test Files

| Test File | Changes |
|---|---|
| `test_models.py` | Remove OpenAI model tests, keep xAI tests |
| `test_normalize.py` | Add tests for new normalizers |
| `test_render.py` | Add tests for new output sections |
| `test_cache.py` | Add tests for report-level cache integration |

### Test Execution

```bash
# Run all tests
python3 -m pytest scripts/tests/ -v

# Run with mock mode (no API calls)
python3 scripts/last30days.py "test topic" --mock --emit=json

# Run specific test module
python3 -m pytest scripts/tests/test_brave_web.py -v
```

---

## Implementation Phases

### Phase 1: Foundation (brave_client + env + models + schema)

**Objective:** Establish the Brave API foundation and updated configuration.

**Files:**
1. Create `scripts/lib/brave_client.py`
2. Modify `scripts/lib/env.py` â€” add BRAVE_API_KEY, update source detection
3. Modify `scripts/lib/models.py` â€” remove OpenAI logic
4. Modify `scripts/lib/schema.py` â€” add NewsItem, VideoItem, DiscussionItem, update Report
5. Create `scripts/tests/test_brave_client.py`
6. Modify `scripts/tests/test_models.py`

**Verification:** All existing tests pass (with updated models tests). New brave_client tests pass.

### Phase 2: Brave Source Modules

**Objective:** Implement all 5 Brave search modules.

**Files:**
1. Create `scripts/lib/brave_web.py`
2. Create `scripts/lib/brave_reddit.py`
3. Create `scripts/lib/brave_news.py`
4. Create `scripts/lib/brave_video.py`
5. Create `scripts/lib/brave_summarizer.py`
6. Create fixtures: `brave_web_sample.json`, `brave_reddit_sample.json`, `brave_news_sample.json`, `brave_video_sample.json`, `brave_summarizer_sample.json`
7. Create tests: `test_brave_web.py`, `test_brave_reddit.py`, `test_brave_news.py`, `test_brave_video.py`, `test_brave_summarizer.py`

**Verification:** All new module tests pass with mock fixtures. No API calls needed.

### Phase 3: Processing Pipeline Updates

**Objective:** Update normalize, score, dedupe for all new sources.

**Files:**
1. Modify `scripts/lib/normalize.py` â€” add new normalizers
2. Modify `scripts/lib/score.py` â€” add new scoring formulas, position-based relevance
3. Modify `scripts/lib/dedupe.py` â€” add cross-source URL dedup, new source dedup functions
4. Modify `scripts/lib/dates.py` â€” UTC consistency fix
5. Modify `scripts/lib/cache.py` â€” wire up report caching
6. Create `scripts/tests/test_score_new.py`
7. Create `scripts/tests/test_dedupe_cross.py`
8. Modify `scripts/tests/test_normalize.py`
9. Modify `scripts/tests/test_cache.py`

**Verification:** Full processing pipeline works with mock data. Scoring formulas produce expected ranges. Dedup correctly handles cross-source duplicates.

### Phase 4: Orchestrator + Output

**Objective:** Rewrite the main orchestrator and output rendering.

**Files:**
1. Rewrite `scripts/last30days.py` â€” new parallel execution, new pipeline, cache integration
2. Modify `scripts/lib/render.py` â€” new output sections for all sources
3. Modify `scripts/lib/ui.py` â€” new progress displays
4. Modify `scripts/tests/test_render.py`
5. Create `scripts/tests/test_integration.py`

**Verification:** `--mock` mode produces complete output with all 7 sources. `--emit=compact|json|md|context|path` all work.

### Phase 5: Interface + Cleanup

**Objective:** Update skill interface, remove deprecated code, final testing.

**Files:**
1. Update `SKILL.md` â€” remove WebSearch, update stats display, simplify flow
2. Update `SPEC.md` â€” reflect new architecture
3. Update `README.md` â€” new installation instructions, examples
4. Remove `scripts/lib/openai_reddit.py`
5. Remove `scripts/lib/websearch.py`
6. Remove `scripts/fixtures/openai_sample.json`
7. Remove `scripts/fixtures/models_openai_sample.json`
8. Final test run: all tests pass

**Verification:** Full skill works end-to-end in mock mode. All tests pass. No references to removed modules.

---

## File Manifest

### Files to Create (11)

| File | Purpose |
|---|---|
| `scripts/lib/brave_client.py` | Shared Brave API HTTP client |
| `scripts/lib/brave_web.py` | General web search + discussions + FAQ + infobox |
| `scripts/lib/brave_reddit.py` | Reddit-focused web search |
| `scripts/lib/brave_news.py` | News search |
| `scripts/lib/brave_video.py` | Video search |
| `scripts/lib/brave_summarizer.py` | AI summarizer (two-step) |
| `scripts/fixtures/brave_web_sample.json` | Mock fixture |
| `scripts/fixtures/brave_reddit_sample.json` | Mock fixture |
| `scripts/fixtures/brave_news_sample.json` | Mock fixture |
| `scripts/fixtures/brave_video_sample.json` | Mock fixture |
| `scripts/fixtures/brave_summarizer_sample.json` | Mock fixture |

### Files to Modify (14)

| File | Changes |
|---|---|
| `scripts/lib/env.py` | Replace OPENAI with BRAVE config, update source detection |
| `scripts/lib/models.py` | Remove OpenAI model logic (~60% reduction) |
| `scripts/lib/schema.py` | Add NewsItem, VideoItem, DiscussionItem, update Report/DataQuality |
| `scripts/lib/normalize.py` | Add 4 new normalizers, update Reddit normalizer |
| `scripts/lib/score.py` | Add 4 new scoring functions, position-based relevance |
| `scripts/lib/dedupe.py` | Add cross-source URL dedup, 4 new source dedup functions |
| `scripts/lib/cache.py` | Wire report caching, remove OpenAI model cache |
| `scripts/lib/render.py` | Add 7 new output sections, update compact/context/full renderers |
| `scripts/lib/ui.py` | Add 4 new progress displays, update promo messages |
| `scripts/lib/dates.py` | UTC consistency fix |
| `scripts/last30days.py` | Complete rewrite: 6-thread parallel, new pipeline, cache integration |
| `SKILL.md` | Remove WebSearch, update flow, update stats display |
| `SPEC.md` | Reflect new architecture |
| `README.md` | New installation, examples, API key instructions |

### Files to Remove (4)

| File | Reason |
|---|---|
| `scripts/lib/openai_reddit.py` | Replaced by `brave_reddit.py` |
| `scripts/lib/websearch.py` | Replaced by `brave_web.py` |
| `scripts/fixtures/openai_sample.json` | No longer applicable |
| `scripts/fixtures/models_openai_sample.json` | No longer applicable |

### Test Files to Create (8)

| File | Coverage |
|---|---|
| `scripts/tests/test_brave_client.py` | BraveClient auth, errors, rate limits |
| `scripts/tests/test_brave_web.py` | Web/discussion/FAQ/infobox parsing |
| `scripts/tests/test_brave_reddit.py` | Reddit URL/subreddit extraction |
| `scripts/tests/test_brave_news.py` | News result parsing |
| `scripts/tests/test_brave_video.py` | Video result parsing |
| `scripts/tests/test_brave_summarizer.py` | Summary/citation parsing |
| `scripts/tests/test_score_new.py` | New scoring formulas |
| `scripts/tests/test_dedupe_cross.py` | Cross-source dedup |

### Test Files to Modify (5)

| File | Changes |
|---|---|
| `scripts/tests/test_models.py` | Remove OpenAI tests |
| `scripts/tests/test_normalize.py` | Add new normalizer tests |
| `scripts/tests/test_render.py` | Add new section tests |
| `scripts/tests/test_cache.py` | Add report cache tests |
| `scripts/tests/test_openai_reddit.py` | Remove entirely (becomes dead code) |

---

## Migration Checklist

### Pre-Implementation

- [ ] Confirm Brave Pro Data AI plan is active and API key works
- [ ] Test Brave API key with a simple curl request
- [ ] Back up current working state (git tag `pre-revamp`)

### Phase 1 Verification

- [ ] `brave_client.py` can make authenticated requests to Brave
- [ ] `env.py` correctly detects BRAVE_API_KEY
- [ ] `models.py` works without OpenAI logic
- [ ] `schema.py` new dataclasses serialize/deserialize correctly
- [ ] All existing tests that don't depend on OpenAI still pass

### Phase 2 Verification

- [ ] Each Brave module parses its fixture correctly
- [ ] `brave_web.py` extracts web + discussions + FAQ + infobox + summarizer key
- [ ] `brave_reddit.py` extracts Reddit URLs and subreddits
- [ ] `brave_news.py` handles up to 50 results
- [ ] `brave_video.py` extracts creator/thumbnail
- [ ] `brave_summarizer.py` handles missing key gracefully

### Phase 3 Verification

- [ ] All normalizers produce correctly typed dataclasses
- [ ] All scoring formulas produce scores in [0, 100] range
- [ ] Cross-source dedup correctly prioritizes Reddit > X > HN > News > Web
- [ ] Report-level cache stores and loads correctly
- [ ] `--refresh` bypasses cache

### Phase 4 Verification

- [ ] `--mock` mode produces output with all 7 sources
- [ ] `--emit=compact` shows all sections
- [ ] `--emit=json` produces valid JSON with all fields
- [ ] `--quick` / default / `--deep` produce appropriate item counts
- [ ] Progress display shows all source phases

### Phase 5 Verification

- [ ] SKILL.md works without WebSearch in allowed-tools
- [ ] No import references to removed modules (`openai_reddit`, `websearch`)
- [ ] All tests pass: `python3 -m pytest scripts/tests/ -v`
- [ ] Mock mode end-to-end: `python3 scripts/last30days.py "test" --mock --emit=compact`
- [ ] Real API test: `python3 scripts/last30days.py "AI coding tools" --emit=compact`

### Post-Implementation

- [ ] Old config detection prints migration message
- [ ] README updated with new installation instructions
- [ ] All raw output files written correctly
- [ ] Cache directory structure is clean
- [ ] No references to OpenAI anywhere in codebase (grep verification)

---

## Cost Analysis

### Current Cost per Query (OpenAI + xAI)

| Component | Cost |
|-----------|------|
| OpenAI GPT-5 web_search (~2K input + ~4K output tokens) | ~$0.06-0.12 |
| xAI grok-4-1 x_search (~2K input + ~4K output tokens) | ~$0.02-0.05 |
| Reddit JSON API (enrichment, ~20 requests) | Free |
| HN Algolia API | Free |
| **Total per query** | **~$0.08-0.17** |

### Revamp Cost per Query (Brave + xAI)

| Component | Cost |
|-----------|------|
| Brave Web Search (2 calls: general + Reddit) @ $9/1K | $0.018 |
| Brave News Search (1 call) | $0.009 |
| Brave Video Search (1 call) | $0.009 |
| Brave Summarizer (1 call) | Free (not billed) |
| Pagination (up to 6 additional calls for default depth) | $0.054 |
| xAI grok-4-1 x_search | ~$0.02-0.05 |
| Reddit JSON API (enrichment) | Free |
| HN Algolia API | Free |
| **Total per query (default depth)** | **~$0.06-0.10** |

**Cost impact:** 10-40% reduction per query with dramatically more data returned.

### Deep Mode Cost

| Depth | Brave API Calls | Brave Cost | xAI Cost | Total |
|-------|-----------------|------------|----------|-------|
| `--quick` | 4 | $0.036 | ~$0.02 | ~$0.056 |
| default | 10 | $0.090 | ~$0.03 | ~$0.120 |
| `--deep` | 16 | $0.144 | ~$0.05 | ~$0.194 |

All within reasonable per-query budgets.

---

## Quality Guarantees

### Zero-Flaw Commitments

1. **Zero hallucinated URLs:** Brave returns real, crawled URLs from its 30B+ page index. No LLM fabrication.
2. **Zero date ambiguity:** Brave provides `page_age` for every result. Combined with `freshness` parameter, date accuracy goes from ~40% to ~99%.
3. **Zero source overlap:** Cross-source URL dedup ensures no duplicate items across Reddit/X/HN/News/Web/Videos/Discussions.
4. **Zero wasted API calls:** Report-level caching (24h TTL) prevents re-fetching identical queries.
5. **Zero configuration complexity:** One Brave API key replaces OpenAI key + model selection + fallback chains.
6. **Zero silent failures:** Every source failure is captured in `report.{source}_error` and displayed to user.
7. **Zero dependency additions:** Brave client uses existing stdlib HTTP (`urllib`). Still zero external dependencies.

### Scoring Integrity

- Social sources (Reddit, X, HN) with verified engagement always rank higher than web/news/video sources at equivalent relevance and recency
- Cross-source dedup preserves highest-engagement version
- Position-based relevance is deterministic and reproducible
- All scores clamped to [0, 100] â€” no overflow, no negative scores

### Data Quality Improvements

| Metric | Current | After Revamp |
|--------|---------|-------------|
| Date accuracy (Reddit) | ~40% | ~99% (Brave page_age) |
| Date accuracy (Web) | ~10% (snippet parsing) | ~99% (Brave page_age) |
| Source diversity | 3 types | 7 types |
| Items per query (default) | ~60-90 | ~150-250 |
| Enrichment data | Engagement only | Engagement + Summary + FAQ + Infobox + Schema |
| Cache hit rate | 0% (not wired) | Expected 20-30% (wired) |
| API call reliability | ~85% (LLM parsing failures) | ~99% (structured JSON) |
